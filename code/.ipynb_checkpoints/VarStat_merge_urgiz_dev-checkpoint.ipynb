{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variability Statistics on full FP LCs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is meant to serve as a place to develop code whose purpose is to provide ugriz extinction-corrected brightness measures (fpsfMeanCorr, N), with a few basic variability parameters ($\\chi^{2}_{DOF}(f)$, $f\\mu_{full}(f)$, $\\sigma_{full}(f)$), saved as FaintFP_NCSA_variability.csv \n",
    "\n",
    "To do that we :\n",
    "- read in 6 cols per patch-filter \n",
    "- merge ugriz data from the patch \n",
    "- add ebv info\n",
    "- correct for extinction\n",
    "- keep only extinction-corrected mean magnitudes, N, and var params\n",
    "- save all patches as  N_objects (rows) x 6 columns * 5 filters as a single file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Necessary imports \n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from pandas import compat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get E(B-V) \n",
    "#DirEBV = '/astro/store/scratch/tmp/suberlak/S13Agg/'\n",
    "#ebv = pd.read_csv(DirEBV+'medianPhotometry.csv', usecols=['objectId','ebv'])\n",
    "\n",
    "\n",
    "Dir = '/astro/store/scratch/tmp/suberlak/S13Agg/repo_fls/'\n",
    "ebv = pd.read_table(Dir+'ebv_NCSA_lt235.dat.gz', delimiter=' ', usecols=[0,1])\n",
    "ebv.columns = ['objectId','ebv']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " a\n"
     ]
    }
   ],
   "source": [
    "print('\\n a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bright_parents_file = 'DeepSourceNCSA_i_lt_235_parent_lt_17_too_bright_narrow.csv'\n",
    "objects_with_bright_parents = pd.read_csv(Dir+bright_parents_file, usecols=[1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "585920"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(objects_with_bright_parents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "585920"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.in1d(ebv['objectId'].values, objects_with_bright_parents['deepSourceId_primary'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_patch(patch='00_21', ebv = ebv, varPatchesDF = None):\n",
    "    \n",
    "    print('Combining ugriz variability results from patch %s'%patch)\n",
    "    \n",
    "    # Directory storing the results of full LC statistics...\n",
    "    DirStat =  '/astro/store/scratch/tmp/suberlak/s13_stripe82/forced_phot_lt_23/NCSA/Var/'\n",
    "    \n",
    "    # only read columns that we use to speed up execution.... \n",
    "    columns = ['objectId','N','chi2DOF','muFull','sigmaFull','psfMean']\n",
    "\n",
    "    # Read in all filters per patch ... \n",
    "    varPatch = {}\n",
    "    for filter in 'ugriz':\n",
    "        File = 'Var'+filter+patch+'.csv'\n",
    "        varPatch[filter] = pd.read_csv(DirStat+File, usecols = columns)\n",
    "\n",
    "    # Check if each patch-filter file has exactly the same number of objects... \n",
    "    for filter in 'ugriz':\n",
    "        print('Number of unique objectId in %s is %d'%(filter, len(np.unique(varPatch[filter]['objectId'].values))))\n",
    "\n",
    "    # add prefix for each filter, apart from the objectId column  \n",
    "    for filter in 'ugriz':\n",
    "        varPatch[filter].columns = [filter+col  if col != 'objectId' else col for col in varPatch[filter]]\n",
    "\n",
    "    # merge ugriz  \n",
    "    howmerge='inner' # to avoid those objects which were in one filter but not in the other...\n",
    "    varPatchug = pd.merge(varPatch['u'], varPatch['g'], how=howmerge, on='objectId', copy=True, indicator=False)\n",
    "    varPatchugr = pd.merge(varPatchug, varPatch['r'], how=howmerge, on='objectId', copy=True, indicator=False)\n",
    "    varPatchiz = pd.merge(varPatch['i'], varPatch['z'], how=howmerge, on='objectId', copy=True, indicator=False)\n",
    "    varPatchugriz = pd.merge(varPatchugr, varPatchiz , how=howmerge, on='objectId', copy=True, indicator=False)\n",
    "\n",
    "    # check how many objects have ugriz photometry and extinction information \n",
    "    withEBV = np.sum(np.in1d(varPatchugriz['objectId'].values, ebv['objectId'].values))\n",
    "    allOBJ = len(varPatchugriz['objectId'].values)\n",
    "    print('Of all %d objects with ugriz info, %d have E(B-V) values from medianPhotometry.csv'%(allOBJ, withEBV))\n",
    "\n",
    "    # Now this can be a left merge - I only want objects that can be extinction-corrected \n",
    "    varPatchAll =pd.merge(varPatchugriz, ebv, how='inner', on='objectId', copy=True, indicator=False)\n",
    "\n",
    "    # Correct for extinction \n",
    "    A = [5.155, 3.793, 2.751, 2.086, 1.479]\n",
    "    filters = 'ugriz'\n",
    "\n",
    "    for i in range(len(A)):\n",
    "        label = filters[i] + 'psfMean'\n",
    "        varPatchAll[label+'_corr'] = varPatchAll[label] +  varPatchAll['ebv'] * A[i]\n",
    "\n",
    "    # Drop unnecessary columns with uncorrected magnitudes.... \n",
    "    compat.PY3 = True\n",
    "    varPatchSave = varPatchAll.drop(['u'+'psfMean'], axis=1)\n",
    "\n",
    "    for filter in 'griz':\n",
    "        varPatchSave = varPatchSave.drop(filter+'psfMean', axis=1)\n",
    "\n",
    "    # add a column saying which patch an object comes from...\n",
    "    varPatchSave['patch'] = patch\n",
    "\n",
    "    \n",
    "   \n",
    "    \n",
    "    if varPatchesDF is not None : \n",
    "        varPatchesDF = varPatchesDF.append(varPatchSave)\n",
    "        \n",
    "    else : \n",
    "        varPatchesDF = varPatchSave\n",
    "        \n",
    "    return varPatchesDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1522762"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(varPatchesDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(14)\n",
    "m = a>6\n",
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-46-63d94b93da63>, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-46-63d94b93da63>\"\u001b[1;36m, line \u001b[1;32m20\u001b[0m\n\u001b[1;33m    varPatchesDF_keep =   objects_with_bright_parents[]\u001b[0m\n\u001b[1;37m                                                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run over patches : \n",
    "varPatchesDF=  process_patch(patch='00_21', ebv = ebv, varPatchesDF = None)\n",
    "\n",
    "# NCSA patches (11)\n",
    "#  '00_21', 22_43', '44_65','66_87', '88_109','110_131', '132_153', '154_175',  '176_181', '365_387', '388_409'\n",
    "\n",
    "# IN2P3 patches (11)\n",
    "#  '155_176', '176_197','197_218', '218_239', '239_260', '260_281', '281_302', \n",
    "#  '302_323','323_344', '344_365', '365_386'\n",
    "\n",
    "for patch in [ '22_43', '44_65','66_87', '88_109','110_131', '132_153', '154_175',  '176_181', '365_387', '388_409' ]:\n",
    "    varPatchesDF=  process_patch(patch=patch, ebv = ebv, varPatchesDF = varPatchesDF)\n",
    "    \n",
    "compat.PY3 = False\n",
    "\n",
    "\n",
    "mask_bright_objects = np.in1d(varPatchesDF['objectId'].values, objects_with_bright_parents['deepSourceId_primary'].values)\n",
    "varPatchesDF_with_bright_parents = varPatchesDF[mask_bright_objects]\n",
    "varPatchesDF_keep =   objects_with_bright_parents[~mask_bright_objects]\n",
    "\n",
    "name = 'test_00_21-44_65_three_patches_ugriz_var.csv'\n",
    "varPatchesDF_keep.to_csv(name)\n",
    "name = \n",
    "varPatchesDF_with_bright_parents.to_csv(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
